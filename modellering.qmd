
# Modellering

## Inleiding


```{r dataread}
library(tidyverse)
library(brms)
library(agrmt)

conflicted::conflicts_prefer(dplyr::filter)


# Functie om kwantielen en gemiddelde te berekenen
quantile_and_m <- function(x, probs = c(0.025, 0.5, 0.975), na.rm = TRUE, avg = TRUE) {
  quantiles <- quantile(x, probs = probs, na.rm = na.rm)
  mean_val <- mean(x, na.rm = na.rm)
  return(if(!avg) quantiles else c(quantiles, avg = mean_val))
}


# laad hulpdata

ess_def <- readr::read_csv2("data/ess_overview.csv", 
                            show_col_types = FALSE) |> 
  left_join(readr::read_csv2("data/cluster_definition.csv", 
                             show_col_types = FALSE),
            join_by(ess_cluster == cluster_name)) |> 
  filter(!is.na(cluster_short), !is.na(ess_order)) |> 
  mutate(ess_short_nospace = str_replace_all(ess_short, " ", ""))

# Laad de data

data_ana <- readr::read_csv2("interim/data_orig.csv") %>%
  select(-ess_order) |> 
  left_join(ess_def, 
            join_by(ess_cluster == ess_cluster,
                    ess_short == ess_short,
                    ess_common == ess_common)) |>
  mutate(scorecum = score + 2, #code score as 1 to 5 instead of -1 to 3
         case = factor(case),
         cluster_short = factor(cluster_short),
         ess_short = factor(ess_short),
         row = row_number()) |> 
  filter(!is.na(score))


ess_short_ordered <- data_ana %>%
  distinct(cluster_short, ess_short) %>%
  arrange(cluster_short, ess_short) %>%
  pull(ess_short)

inbo_palette <- INBOtheme::inbo_palette()

data_agg <- data_ana %>%
  group_by(case, ess_cluster, cluster_short, ess_common, ess_short) %>%
  summarise(mean_score = mean(score, na.rm = TRUE),
            n_respondents = n(),
            .groups = "drop")

data_cum <- data_ana |> 
  group_by(ess_cluster, ess_common, ess_short, case, score) |> 
  summarise(n_respondents = n(), .groups = "drop_last") |> 
  mutate(fraction = n_respondents / sum(n_respondents),
         isnegative = ifelse(score < 0, 1, 0),
         fscore = ifelse(score < 0, NA, score + 1),
         nresp = n_respondents) #nodig voor brms zonder underscores

```

## Analyses op dataset zelf


### similariteitsprofiel

```{r similarity}
library(cluster)
library(dplyr)



# Assuming your data is in long format: region, instance, person, score
# First reshape to get mean scores by region and instance
case_profiles <- data_ana %>%
  group_by(case, ess_short) %>%
  summarise(mean_score = mean(score, na.rm = TRUE), .groups = 'drop') %>%
  pivot_wider(names_from = ess_short, values_from = mean_score)

# Create matrix with regions as rows, instances as columns
profile_matrix <- as.matrix(case_profiles[, -1])
rownames(profile_matrix) <- case_profiles$case

col_means <- colMeans(profile_matrix, na.rm = TRUE)
profile_detrended <- sweep(profile_matrix, 2, col_means, "-")
profile_detrended[is.na(profile_detrended)] <- 0 # Replace NA with 0 for distance calculation

# Calculate similarity/distance matrix
similarity_matrix <- 1 - as.matrix(dist(profile_detrended, method = "euclidean")) / max(dist(profile_detrended, method = "euclidean"))


library(corrplot)
library(pheatmap)

pheatmap(similarity_matrix,
         clustering_distance_rows = "euclidean",
         clustering_distance_cols = "euclidean",
         color = colorRampPalette(c("red", "white", "blue"))(100),
         main = "Regional Similarity Matrix")


#Nu eens omgekeerd
profile_matrix_t <- t(profile_matrix)
col_means <- colMeans(profile_matrix_t, na.rm = TRUE)
profile_detrended_t <- sweep(profile_matrix_t, 2, col_means, "-")
profile_detrended_t[is.na(profile_detrended_t)] <- 0 # Replace NA with 0 for distance calculation

similarity_matrix_t <- 1 - as.matrix(dist(profile_detrended_t, method = "euclidean")) / max(dist(profile_detrended_t, method = "euclidean"))

pheatmap(similarity_matrix_t,
         clustering_distance_rows = "euclidean",
         clustering_distance_cols = "euclidean",
         color = colorRampPalette(c("red", "white", "blue"))(100),
         main = "Regional Similarity Matrix")




library(igraph)
library(ggraph)

# Create network from high similarities (threshold = 0.6, adjust as needed)
network_matrix <- similarity_matrix
network_matrix[network_matrix < 0.3] <- 0
diag(network_matrix) <- 0

# Create network
network <- graph_from_adjacency_matrix(network_matrix, 
                                     weighted = TRUE, 
                                     mode = "undirected")

# Network plot
ggraph(network, layout = "fr") + 
  geom_edge_link(aes(width = weight, alpha = weight)) +
  geom_node_point(size = 4, color = "steelblue") +
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +
  scale_edge_width(range = c(0.5, 3)) +
  theme_void() +
  labs(title = "Regional Similarity Network")


mds <- cmdscale(1 - similarity_matrix, k = 2)
mds_df <- data.frame(
  region = rownames(similarity_matrix),
  MDS1 = mds[, 1],
  MDS2 = mds[, 2]
)

# Plot MDS
ggplot(mds_df, aes(x = MDS1, y = MDS2, label = region)) +
  geom_point(size = 3, color = "steelblue") +
  geom_text(hjust = 0, vjust = 0, nudge_x = 0.05, nudge_y = 0.05) +
  theme_minimal() +
  labs(title = "Regional Similarity - MDS Plot",
       x = "Dimension 1", y = "Dimension 2")



# Summary statistics
summary_stats <- data.frame(
  region = rownames(similarity_matrix),
  mean_similarity = rowMeans(similarity_matrix, na.rm = TRUE),
  max_similarity = apply(similarity_matrix, 1, function(x) max(x[x != 1], na.rm = TRUE)),
  min_similarity = apply(similarity_matrix, 1, min, na.rm = TRUE)
)

print(summary_stats)

# Find most similar pairs
similarity_pairs <- which(similarity_matrix > quantile(similarity_matrix[upper.tri(similarity_matrix)], 0.7), 
                         arr.ind = TRUE)
similar_regions <- data.frame(
  region1 = rownames(similarity_matrix)[similarity_pairs[,1]],
  region2 = colnames(similarity_matrix)[similarity_pairs[,2]],
  similarity = similarity_matrix[similarity_pairs]) |> 
    filter(region1 < region2) |> 
  arrange(desc(similarity))
print(similar_regions)





analyze_similarity_matrix <- function(sim_matrix, threshold = 0.6) {
  
  # 1. Basic stats
  cat("Similarity Matrix Analysis\n")
  cat("=========================\n")
  cat("Mean similarity:", round(mean(sim_matrix[upper.tri(sim_matrix)]), 3), "\n")
  cat("Range:", round(range(sim_matrix[upper.tri(sim_matrix)]), 3), "\n\n")
  
  # 2. Most similar regions
  max_sim <- which(sim_matrix == max(sim_matrix[sim_matrix < 1]), arr.ind = TRUE)[1,]
  cat("Most similar regions:", rownames(sim_matrix)[max_sim[1]], "and", 
      colnames(sim_matrix)[max_sim[2]], 
      "(similarity =", round(sim_matrix[max_sim[1], max_sim[2]], 3), ")\n\n")
  
  # 3. Clustering
  hc <- hclust(as.dist(1 - sim_matrix), method = "ward.D2")
  clusters <- cutree(hc, k = 3)
  
  cat("Cluster assignments:\n")
  for(i in 1:3) {
    cat("Cluster", i, ":", names(clusters)[clusters == i], "\n")
  }
  
  # Return useful objects
  list(
    clusters = clusters,
    hclust = hc,
    summary_stats = data.frame(
      region = rownames(sim_matrix),
      mean_sim = rowMeans(sim_matrix),
      cluster = clusters
    )
  )
}

# Use the function
results <- analyze_similarity_matrix(similarity_matrix)
plot(results$hclust)

```


### Consensus

Hoe de mensen een ecosysteemdienst  waarderen, is niet voor iedere case dezelfde en verschilt ook van respondent tot respondent. We kunnen de overeenstemming tussen respondenten binnen een case bekijken door de Leik's agreement score te berekenen. Deze score geeft aan hoe goed de respondenten het eens zijn over de waardering van een ecosysteemdienst.

De keuze is gevallen op de Leik score omdat deze score goed werkt voor ordinale data en de scores van -1 tot 3 zijn ordinaal. De Leik score is een maat voor de overeenstemming tussen respondenten, waarbij 0 betekent dat er geen overeenstemming is en 1 betekent dat er volledige overeenstemming is. 

Dus als iedereen dezelfde score geeft, is de Leik score 1. Als de helft het minimum als score geeft en de andere helft het maximum, is de Leik score 0. De Leik score zal het beoordelen van naburige scores minder sterk afstraffen dan als de klasses verder uiteen liggen. Een Leik score van 0.5 krijg je bijvoorbeeld als de helft de tweede en de andere helft de vierde score geeft in een scoresysteem met 5 klassen.

```{r Leik_agreement}
#| caption: "Leik's Agreement Scores for ESS Common"
mat <- rbind(c(0,0,20,0,0),
             c(0,5,10,5,0),
             c(0,10,10,0,0),
             c(0,7,7,7,0),
             c(0,10,0,10,0),
             c(7,0,7,0,7),
             c(10,0,0,0,10))
colnames(mat) <- c(-1, 0, 1, 2, 3)
mat <- bind_cols(mat, Leikscore = apply(mat, 1, function(x) 1 - agrmt::Leik(x)))
knitr::kable(mat, digits = 2)

```


```{r agreement_general} 
#NAKIJKEN waarom in kluis dynamic r&r en anderen een score groter dan 1 geven
data_agree <- data_ana |> 
  group_by(ess_short, cluster_short, score, case) |> 
  summarise(n_respondents = n(), .groups = "drop_last") |> 
  summarise(n_respondents = mean(n_respondents, .groups = "drop")) 

data_agree_case <- data_ana |> 
  group_by(ess_short, cluster_short, score, case) |> 
  summarise(n_respondents = n(), .groups = "drop") 



data_consensus_case <- data_agree_case |>
  group_by(ess_short, cluster_short, case) |>
  arrange(score) |>
  mutate(ess_short = factor(ess_short, levels = ess_short_ordered)) |> 
  summarise(agreement = 1 - Leik(n_respondents), .groups = "drop")

data_consensus <- data_agree |>
  group_by(ess_short, cluster_short) |>
  arrange(score) |>
  mutate(ess_short = factor(ess_short, levels = ess_short_ordered)) |> 
  summarise(agreement = 1 - Leik(n_respondents), .groups = "drop")


ggplot(data_consensus, aes(x = ess_short, y = agreement, color = cluster_short)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  scale_color_manual(values = inbo_palette) +
  labs(title = "Agreement Scores by ESS Common", x = "ESS Common", y = "Agreement Score")


ggplot(data_consensus_case, aes(x = ess_short, y = agreement, color = cluster_short)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  scale_color_manual(values = inbo_palette) +
  labs(title = "Agreement Scores by ESS Common", x = "ESS Common", y = "Agreement Score") +
  facet_wrap(~case)







```



## Modellen zonder respondent-level informatie

We gaan een aantal versimpelde modellen maken, waarbij we de respondent-level informatie negeren en alleen werken met de case-level informatie. Dit is handig voor een eerste verkenning van de data en om te zien of er duidelijke patronen zijn.


### Gewogen gemiddelden

In het gewogen gemiddelde model, beschouwen we de gemiddelde score per ecosysteemdienst, gewogen naar het aantal respondenten per case. Dit model is eenvoudig en geeft een eerste indicatie van de gewensteheid van de ecosysteemdiensten.

Hiervoor wordt een eenvoudig gaussiaans random intercept model gebruikt, alhoewel via een bayesiaanse schatting via het `brms` package.

De reden voor een bayesiaanse schatting is dat we de onzekerheid en predicties rond de schattingen eenvoudiger kunnen bepalen dan met een standaard lineair mixed model in het `lme4`package.

```{r simplified_weighted_means}
#| cache: true
#| echo: true
# aggregated_summary_data has one row per ess_common per case
# sigma on estimates modelled to n_respondents (variance: sig2/n -> sigma ~ 1/sqrt(n))
conflicted::conflict_prefer_all(c("dplyr"), quiet = TRUE)
conflicted::conflict_prefer_all(c("purrr"), quiet = TRUE)
conflicted::conflict_prefer_all(c("tidyr"), quiet = TRUE)
conflicted::conflict_prefer_all(c("brms"), quiet = TRUE)

model_simpmeans <- brm(
  mean_score | weights(n_respondents) ~ 0 + ess_short + (1 | case),
  data = data_agg,
  family = gaussian(),
  prior = c(set_prior("normal(0, 5)", class = "b"),
            set_prior("cauchy(0, 2)", class = "sigma")),
  chains = 4,
  cores = 4,
  iter = 3000,
  warmup = 1000
)
save(model_simpmeans, file = "models/simplified_weighted_means.RData")
```


```{r swm_res}
#| echo: false
#| fig.caption: "Residuals vs Fitted Values for the Simplified Weighted Means Model"
invisible(summary(model_simpmeans))
r <- residuals(model_simpmeans)[,"Estimate"]
f <- fitted(model_simpmeans)[,"Estimate"]

ggplot(data.frame(res = r, fit = f), aes(x = fit, y = res)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")
# 
# #drought is not yet in the ess definition !!! CHECK
# conditional_eff <- conditional_effects(model_simpmeans, effects = "ess_short")$ess_short |> 
#   mutate(ess_short = reorder(ess_short, estimate__, mean, decreasing = TRUE)) |> 
#   left_join(ess_def |> filter(is.na(remark)), join_by("ess_short" == "ess_short"))
# 
# ggplot(conditional_eff, aes(x = ess_short, y = estimate__, color = cluster_short)) +
#   geom_point() +
#   geom_errorbar(aes(ymin = lower__, ymax = upper__)) +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
#   labs(title = "Conditional Effects of ESS Common", x = "ESS Common", y = "Estimated Effect")
# 

```


De plot hieronder toont de geschatte effecten van de ecosysteemdiensten. Enkel gemotoriseerde recreatie wordt als ongewenst gezien, alsook jacht. De andere ecosysteemdiensten worden als gewenst gezien. Vooral biodiversiteit, levenskwaliteit en waterhuishouding worden heel positief ingeschat.

Recreatie en toerisme worden heel variabel ingeschat, van heel negatief zoals gemotoriseerde recreatie tot heel positief zoals de mogelijkheid tot spelen en zachte recreatie zoals wandelen.

```{r swm_pred}
#| echo: false
#| fig.caption: "Estimated Effects of ESS with the  95% Uncertainty intervals"
coeftable <- fixef(model_simpmeans) |> 
  as.data.frame() |> 
  rownames_to_column("ess_short_brms") |> 
  mutate(ess_short_brms = str_replace(ess_short_brms, "ess_short", "")) |> 
  left_join(ess_def, join_by(ess_short_brms == ess_short_nospace)) |> 
  mutate(ess_short = factor(ess_short, levels = ess_short_ordered))

ggplot(coeftable, aes(x = ess_short, y = Estimate, color = cluster_short)) +
  geom_point() +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(title = "Estimated Effects of ESS Common",
       x = "ESS Common",
       y = "Estimated Effect",
       color = "Cluster") +
  scale_color_manual(values = inbo_palette)




#introduceer de agreement as sizes
coeftable_consensus <- coeftable |> 
  left_join(data_consensus |> 
              select(ess_short, cluster_short, agreement), 
            by = c("ess_short" = "ess_short", "cluster_short" = "cluster_short"))


ggplot(coeftable_consensus, aes(x = ess_short, y = Estimate, color = agreement)) +
  geom_point(aes(size = agreement), alpha = 1) +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(title = "Estimated Effects of ESS Common colored en resized with Agreement",
       x = "ESS Common",
       y = "Estimated Effect",
       color = "Agreement", 
       size = "Agreement") +
  #scale_color_manual(values = inbo_palette) + 
  scale_color_gradient(low = "cyan", high = "blue") +
  scale_size_continuous(range = c(2, 8))




newdata <- data_agg |> 
  transmute(ess_short, cluster_short, case) |>
  distinct() |> 
  mutate(rownr = row_number())

coeftable_case <- fitted(model_simpmeans, newdata = newdata, summary = TRUE) |> 
  as.data.frame() |> 
  mutate(ess_short = factor(newdata |> pull(ess_short), levels = ess_short_ordered),
         cluster_short = newdata |> pull(cluster_short),
         case = newdata |> pull(case)) |> 
  left_join(data_consensus_case |> 
              select(ess_short, cluster_short, case, agreement), 
            by = c("ess_short" = "ess_short", "cluster_short" = "cluster_short", "case" = "case"))

ggplot(coeftable_case, aes(x = ess_short, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = agreement)) +
  geom_point() +
  geom_errorbar() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  scale_color_gradient(low = "cyan", high = "blue") +
  labs(title = "Estimated Effects of ESS Common per Case, colored by agreement",
       x = "ESS Common",
       y = "Estimated Effect") +
  facet_wrap(~case, nrow = 3)

```


Hoe ecosysteemdiensten scoren zijn niet voor iedere case hetzelfde. We kunnen de random effecten van de cases bekijken om te zien hoe de geschatte effecten per case verschillen van het gemiddelde effect.

De hoofdreden hier is waarschijnlijk dat niet alle ecosysteemdiensten even zichtbaar aanwezig zijn, of minder belangrijk zijn doordat de case (regio) sowieso al meerdere ecosysteemdiensten heeft in nabijgelegen gebieden.


```{r swm_ran}
random_effects <- ranef(model_simpmeans)
ranef_case <- random_effects$case[,,"Intercept"] |>
  as_tibble(rownames = "case") |> 
  mutate(case = reorder(case, Estimate, mean, decreasing = TRUE))

ggplot(ranef_case, aes(x = case, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_point() +
  geom_errorbar() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(title = "Random Effects for Cases", x = "Case", y = "Random Effect Estimate")

```


### Cumulatief logistisch model

Dit is volgens mij het meest correcte model om de data te modelleren.
Er worden geen aannames gedaan over de verdeling van de data.
Er wordt berekend wat de cumulatieve kans is op een score. Dit betekent dat het model de kans berekent dat een ecosysteemdienst een bepaalde score of lager heeft, in plaats van een specifieke score.

```{r simplfied_cumlogit_model}
#| cache: true
#ignore resp+ondent-level information for 9/14 regions
# Simplified cumulative logit model
model_simpcl <- brm(
  scorecum ~ ess_short + (1 | case),
  data = data_ana,
  family = cumulative("logit"),
  chains = 4,
  iter = 2000,
  cores = 4
)
save(model_simpcl, file = "models/simplified_cumlogit_model.RData")
```

```{r simpcl_pred}

invisible(summary(model_simpcl))
#plot(model_simpcl)


# Create newdata for each ess_common
newdata_simpcl <- data_ana |> transmute(ess_short, cluster_short,  case = "global") |>
  distinct() |> 
  mutate(rownr = row_number())

fitted_summary <- fitted(model_simpcl, newdata = newdata_simpcl, summary = TRUE, allow_new_levels = TRUE)
fitted_summary_s <-  fitted(model_simpcl, newdata = newdata_simpcl, summary = FALSE, allow_new_levels = TRUE)
summary_01 <- fitted_summary_s[,,1]
summary_12 <- fitted_summary_s[,,1] + fitted_summary_s[,,2]
summary_13 <- fitted_summary_s[,,1] + fitted_summary_s[,,2] + fitted_summary_s[,,3]
summary_14 <- fitted_summary_s[,,1] + fitted_summary_s[,,2] + fitted_summary_s[,,3] + fitted_summary_s[,,4]
summary_15 <- fitted_summary_s[,,1] + fitted_summary_s[,,2] + fitted_summary_s[,,3] + fitted_summary_s[,,4] + fitted_summary_s[,,5]



pred01 <- t(apply(summary_01, 2, quantile_and_m, probs = c(0.025,0.500,0.975), na.rm = TRUE)) |> 
  as.data.frame() |> 
  mutate(value = "P(-1)", 
         ess_short = newdata_simpcl |> pull(ess_short),
         cluster_short = newdata_simpcl |> pull(cluster_short))

pred12 <- t(apply(summary_12, 2, quantile_and_m, probs = c(0.025,0.500,0.975), na.rm = TRUE)) |> 
  as.data.frame() |> 
  mutate(value = "P(<=0)", 
         ess_short = newdata_simpcl |> pull(ess_short),
         cluster_short = newdata_simpcl |> pull(cluster_short))

pred13 <- t(apply(summary_13, 2, quantile_and_m, probs = c(0.025,0.500,0.975), na.rm = TRUE)) |> 
  as.data.frame() |> 
  mutate(value = "P(<=1)", 
         ess_short = newdata_simpcl |> pull(ess_short),
         cluster_short = newdata_simpcl |> pull(cluster_short))

pred14 <- t(apply(summary_14, 2, quantile_and_m, probs = c(0.025,0.500,0.975), na.rm = TRUE)) |> 
  as.data.frame() |> 
  mutate(value = "P(<=2)", 
         ess_short = newdata_simpcl |> pull(ess_short),
         cluster_short = newdata_simpcl |> pull(cluster_short))

pred15 <- t(apply(summary_15, 2, quantile_and_m, probs = c(0.025,0.500,0.975), na.rm = TRUE)) |> 
  as.data.frame() |> 
  mutate(value = "P(<=3)", 
         ess_short = newdata_simpcl |> pull(ess_short),
         cluster_short = newdata_simpcl |> pull(cluster_short))

predicties_clm <- bind_rows(pred01, pred12, pred13, pred14, pred15) |> 
  mutate(value = factor(value, levels = rev(c("P(-1)", "P(<=0)", "P(<=1)", "P(<=2)", "P(<=3)"))))

estimates_clm <- fitted_summary[,1,] |> 
  as.data.frame() |> 
  mutate(ess_short = newdata_simpcl |> pull(ess_short),
         cluster_short = newdata_simpcl |> pull(cluster_short)) |> 
  pivot_longer(cols = -c(ess_short, cluster_short), names_to = "value", values_to = "estimate") |> 
  mutate(value = factor(value,
                        levels = c("P(Y = 5)", "P(Y = 4)", "P(Y = 3)", "P(Y = 2)", "P(Y = 1)"),
                        labels = c("P(<=3)","P(<=2)","P(<=1)","P(<=0)","P(-1)")))
         


ggplot(estimates_clm, aes(x = ess_short, y = estimate, fill = value)) +
  geom_bar(stat = "identity", position = "stack") + 
  geom_errorbar(data = predicties_clm, 
                aes(ymin = `2.5%`, ymax = `97.5%`, x = ess_short, y = `50%`),
                position = position_dodge(0.8)) +
  geom_point(data = predicties_clm, 
             aes(x = ess_short, y = avg), 
             position = position_dodge(0.8)) +
  facet_wrap(~cluster_short, scales = "free_x", nrow = 2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.3, vjust = 0.5)) +
  scale_fill_manual(values = rev(RColorBrewer::brewer.pal(5, "RdYlBu"))) + 
  labs(title = "Estimated score of ESS  with 95% Uncertainty Intervals on cumulative value",
       x = "ESS",
       y = "Estimated Probability",
       fill = "Score")
ggsave("models/simplified_cumlogit_model_estimates.png", width = 7, height = 10)
```

```{r simpcl_ran}

VarCorr(model_simpcl)
random_effects <- ranef(model_simpcl)
# Extract random effects for each case
ranef_case <- random_effects$case[,,"Intercept"] |>
  as_tibble(rownames = "case") |> 
  mutate(case = reorder(case, Estimate, mean, decreasing = TRUE))

ggplot(ranef_case, aes(x = case, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_point() +
  geom_errorbar() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(title = "Random Effects for Cases", x = "Case", y = "Random Effect Estimate") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```



```{r simpcl_consensus}

pred_probs <- fitted(model_simpcl, newdata = newdata_simpcl, summary = FALSE, allow_new_levels = TRUE)

consensus_scores <- apply(pred_probs, c(1,2), function(x) 1 - agrmt::Leik(x)) |> 
  as_tibble()
colnames(consensus_scores) <- unique(data_ana$ess_short)
consensus_scores <- pivot_longer(consensus_scores, cols = everything(), names_to = "ess_short", values_to = "consensus_score") 
consensus_scores <- consensus_scores |> 
  left_join(ess_def |> 
              select(ess_short, cluster_short), 
            by = "ess_short") |> 
  mutate(ess_short = factor(ess_short, levels = ess_short_ordered))


q_m <- function(x) as_tibble(t(quantile_and_m(x)))

consensus_scores_plt <- consensus_scores |> 
  group_by(ess_short, cluster_short) |> 
  reframe(q_m(consensus_score))

ggplot(consensus_scores_plt, aes(x = ess_short, y = avg, ymin = `2.5%`, ymax = `97.5%`, color = cluster_short)) +
  geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  ylim(0, 1) +
  labs(title = "Consensus Scores by ESS Common", x = "ESS Common", y = "Consensus Score")


consensus_mean_scores <- consensus_scores |> 
  group_by(ess_short, cluster_short) |> 
  summarise(consensus_mean = mean(consensus_score, na.rm = TRUE), .groups = "drop") |> 
  mutate(ess_short = factor(ess_short, levels = ess_short_ordered))

avgscores <- fitted_summary[, "Estimate", ] |> 
  as.data.frame() |>
  mutate(ess_short = newdata_simpcl |> pull(ess_short),
         cluster_short = newdata_simpcl |> pull(cluster_short)) |> 
  rowwise() |>
  mutate(avg = sum(c(-1:3) * c_across(starts_with("P(Y ="))))

plotdata <- consensus_mean_scores |> 
  left_join(avgscores, by = c("ess_short", "cluster_short")) 


ggplot(plotdata, aes(x = ess_short, y = avg, color = cluster_short, size = consensus_mean)) +
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
  labs(title = "Average Scores and Consensus Scores by ESS Common",
       x = "ESS Common",
       y = "Average Score",
       color = "Cluster",
       size = "Consensus") +
  scale_size_continuous(range = c(4, 8))



```


```{r simpcl_consensus_case}

newdata_case <- data_ana |> 
  transmute(ess_short, cluster_short, case) |>
  distinct() |> 
  mutate(rownr = row_number())

pred_probs_case <- fitted(model_simpcl, newdata = newdata_case, summary = FALSE, allow_new_levels = TRUE)
# Calculate consensus scores for each case

pred_probs_case_s <- fitted(model_simpcl, newdata = newdata_case, summary = TRUE, allow_new_levels = TRUE)

avgscores_case <- pred_probs_case_s[, "Estimate", ] |> 
  as.data.frame() |>
  mutate(ess_short = newdata_case |> pull(ess_short),
         cluster_short = newdata_case |> pull(cluster_short),
         case = newdata_case |> pull(case))|> 
  rowwise() |>
  mutate(avg = sum(c(-1:3) * c_across(starts_with("P(Y ="))))




leik_case <- NULL
for (j in 1:ncol(pred_probs_case)) {
  #print(j)
  scores <- pred_probs_case[, j, ]
  score <- apply(scores, 1, function(x) 1 - Leik(x))
  avg_score <- mean(score, na.rm = TRUE)
  q025_score <- quantile(score, 0.025, na.rm = TRUE)
  q075_score <- quantile(score, 0.975, na.rm = TRUE)
  rv <- data.frame(ess_short = newdata_case$ess_short[j],
                   cluster_short = newdata_case$cluster_short[j],
                   case = newdata_case$case[j],
                   avg_leik = avg_score,
                   q025_leik = q025_score,
                   q975_leik = q075_score)
  leik_case <- bind_rows(leik_case, rv)
}


ggplot(leik_case, aes(x = ess_short, y = avg_leik, ymin = q025_leik, ymax = q975_leik, color = cluster_short)) +
  geom_point() +
  geom_errorbar() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(title = "Consensus Scores by ESS Common and Case", x = "ESS Common", y = "Consensus Score") +
  scale_color_manual(values = inbo_palette) +
  facet_wrap(~case)


#nu nog de globale gemiddeldes toevoegen (errorbars kunnen hier niet, maar zouden ook nog kunnen)

plotdata_case <- avgscores_case |> 
  left_join(leik_case, by = c("ess_short", "cluster_short", "case")) 

ggplot(plotdata_case, aes(x = ess_short, y = avg, color = cluster_short, size = avg_leik)) +
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
  labs(title = "Average Scores and Consensus Scores by ESS Common and Case",
       x = "ESS Common",
       y = "Average Score",
       color = "Cluster",
       size = "Consensus") +
  scale_size_continuous(range = c(2, 4)) +
  facet_wrap(~case)

```






### Hurdle model

```{r simplified_hurdle_model}
#| cache: true
model_simphurdle <- brm(
    bf(isnegative ~ ess_short + (1|case),
       nresp  ~ score * ess_short + (1|case)),
    data = data_cum, # same expanded data as above
    family = hurdle_negbinomial(),
    chains = 4,
    iter = 2000,
    cores = 4
    )
save(model_simphurdle, file = "models/simplified_hurdle_model.RData")

```


## Volledige modellen

We gaan nu de volledige modellen opstellen, waarbij we rekening houden met de respondent-level informatie en de verschillende regio's. We zullen zowel een cumulatief logistisch model als een hurdle model opstellen.


### Cumulatief logistisch model


```{r cumlogit_model}

model <- brm(
  score_cumlogit ~ ess_short + (ess_short | case) + (1 | respondent),
  data = data_ana,
  family = cumulative("logit"),
  chains = 4,
  iter = 2000,
  cores = 4
)

save(model, file = "models/cumlogit_first_model.RData")
```

### Hurdle model

```{r hurdle_model}
hurdle_model <- brm(
  bf(
    # Hurdle part: probability of non-negative rating
    ispositive ~ ess_short + (ess_short | case) + (1 | respondent),
    # Positive part: level of positivity given non-negative
    scorehu ~ ess_short + (ess_short | region) + (1 | respondent)
  ),
  data = combined_data,
  family = hurdle_negbinomial(),
  chains = 4,
  iter = 2000,
  cores = 4
)
save(hurdle_model, file = "models/hurdle_first_model.Rdata")
```


## Modelformulering vooor hurdle model met respondent-level informatie

```{r}

#Eerst nog eens thesis Ilja nalezen

data_ana <- data_processed %>%
  mutate(score = score + 1) # Omgaan met nullen in de score
#Let op, data nog aanpassen aan het feit dat er net altijd een respondent gekend is, en dat er moet gewogen worden voor de cases met het aantal respondenten


#Op subset van de data voor enkel de cases waarvoor respondenten gekend zijn

library(brms)
# Definieer het model
model <- brm(
  bf(
    # Binaire component
    score ~ 1,
    # Negatief binomiaal component
    score ~ ess_short + (1|case/respondent),
    family = hurdle_negbinomial()
  ),
  data = data,
  prior = c(
    set_prior("normal(0, 5)", class = "b"),
    set_prior("normal(0, 5)", class = "Intercept")
  ),
  chains = 4,
  iter = 2000,
  warmup = 1000,
  control = list(adapt_delta = 0.95)
)
# Samenvatting van het model
summary(model)
# Visualiseer de resultaten
plot(model)
# Voorspel de waarden
predictions <- posterior_predict(model)

```


<!--
# Visualiseer de voorspellingen
hist(predictions, main = "Posterior Predictions", xlab = "Predicted Values", breaks = 30)
# Opslaan van het model
saveRDS(model, file = "hurdle_model.rds")
# Laad het model
loaded_model <- readRDS("hurdle_model.rds")
# Voorspel nieuwe waarden
new_data <- data.frame(region = unique(data$region))
predictions_new <- posterior_predict(loaded_model, newdata = new_data)
# Visualiseer de nieuwe voorspellingen
hist(predictions_new, main = "Posterior Predictions for New Data", xlab = "Predicted Values", breaks = 30)
# Opslaan van de voorspellingen
write.csv(predictions_new, file = "predictions_new.csv", row.names = FALSE)
# Evaluatie van het model
library(loo)
loo_result <- loo(model)
# Samenvatting van de LOO-criteria
print(loo_result)
# Visualiseer de LOO-criteria
plot(loo_result)
# Opslaan van de LOO-criteria
saveRDS(loo_result, file = "loo_result.rds")
# Laad de LOO-criteria
loaded_loo_result <- readRDS("loo_result.rds")
# Visualiseer de geladen LOO-criteria
plot(loaded_loo_result)
# Opslaan van de resultaten
saveRDS(list(model = model, predictions = predictions, loo_result = loo_result), file = "model_results.rds")
# Laad de resultaten
loaded_results <- readRDS("model_results.rds")
# Visualiseer de geladen resultaten
plot(loaded_results$model)
# Voorspel de waarden met de geladen resultaten
predictions_loaded <- posterior_predict(loaded_results$model)
# Visualiseer de voorspellingen van de geladen resultaten
hist(predictions_loaded, main = "Posterior Predictions from Loaded Model", xlab = "Predicted Values", breaks = 30)
# Opslaan van de voorspellingen van de geladen resultaten
write.csv(predictions_loaded, file = "predictions_loaded.csv", row.names = FALSE)
# Evaluatie van de voorspellingen
library(ggplot2)
ggplot(data = data.frame(predictions = predictions_loaded), aes(x = predictions)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  labs(title = "Histogram of Predictions from Loaded Model", x = "Predicted Values", y = "Frequency") +
  theme_minimal()
# Opslaan van de visualisatie
ggsave("predictions_histogram.png", width = 8, height = 6)
# Laad de visualisatie
predictions_histogram <- png::readPNG("predictions_histogram.png")
# Visualiseer de geladen histogram
grid::grid.raster(predictions_histogram)
# Opslaan van de geladen histogram
png::writePNG(predictions_histogram, "predictions_histogram_loaded.png")
# Laad de geladen histogram
loaded_predictions_histogram <- png::readPNG("predictions_histogram_loaded.png")
# Visualiseer de geladen histogram
grid::grid.raster(loaded_predictions_histogram)
# Opslaan van de geladen histogram
png::writePNG(loaded_predictions_histogram, "predictions_histogram_final.png")
# Laad de finale histogram
final_predictions_histogram <- png::readPNG("predictions_histogram_final.png")
# Visualiseer de finale histogram
grid::grid.raster(final_predictions_histogram)
# Opslaan van de finale histogram
png::writePNG(final_predictions_histogram, "predictions_histogram_final.png")
# Laad de finale histogram
final_predictions_histogram <- png::readPNG("predictions_histogram_final.png")
# Visualiseer de finale histogram
grid::grid.raster(final_predictions_histogram)
# Opslaan van de finale histogram
png::writePNG(final_predictions_histogram, "predictions_histogram_final.png")
# Laad de finale histogram
final_predictions_histogram <- png::readPNG("predictions_histogram_final.png")
# Visualiseer de finale histogram
grid::grid.raster(final_predictions_histogram)
# Opslaan van de finale histogram
```
-->



